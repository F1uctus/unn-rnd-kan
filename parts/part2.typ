#import "../lib.typ": *

#part_count.step()

= Интерпретируемость <ch2>

== Разреживание <ch2:sec1>

Неизвестно, как выбрать форму сети, которая наилучшим образом соответствует структуре набора данных. Например, если мы знаем, что набор данных генерируется по формуле $f(x,y)=exp(sin(pi x)+y^2)$, то сеть с сигнатурой $[2, 1, 1]$ способна выразить эту функцию. Однако на практике мы не знаем этого априори, поэтому было бы неплохо иметь подход для автоматического определения оптимальной формы сети. Можно начать с достаточно большой сети и обучить её с использованием регуляризации структурной разреженности и последующим процессом прореживания (pruning). Покажем, что такая сеть гораздо более интерпретируема, чем непрореженная.

Для MLP регуляризация с помощью $L_1$-нормы весов используется для увеличения разреженности. Можно адаптировать этот метод, применив две модификации:

1. Линейные веса заменены обучаемыми функциями активации, поэтому мы должны определить $L_1$-норму функций активации.
2. $L_1$-норма недостаточна для прореживания; вместо этого требуется дополнительная энтропийная регуляризация.

Определим норму функции активации $phi.alt$ как её среднюю величину по всем $N_p$ входам.

#definition(title: "Норма функции активации")[
$ norm(phi.alt)_1 = 1/N_p
                    sum_(s=1)^(N_p)
                    abs(phi.alt(x^"(s)"))
. $ <eq:l1act>
]

Тогда для слоя $Phi$ с $n_"in"$ входами и $n_"out"$ выходами определим норму $Phi$ как сумму норм всех функций активации.

#definition(title: "Норма слоя")[
$ norm(Phi)_1 = sum_(i=1)^(n_"in")
                sum_(j=1)^(n_"out")
                norm(phi.alt_(i,j))_1
. $ <eq:l1layer>
]

Кроме того, определим энтропию $Phi$.

#definition(title: "Энтропия слоя")[
$ S(Phi) = - sum_(i=1)^(n_"in")
             sum_(j=1)^(n_"out")
             norm(phi.alt_(i,j))_1 / norm(Phi)_1
             log( norm(phi.alt_(i,j))_1 / norm(Phi)_1 )
. $ <eq:layer-entropy>
]

Общая функция потерь $ell_"total"$ включает ошибку предсказания $ell_"pred"$,
L1-регуляризацию и энтропийную регуляризацию всех слоёв:

*Функция потерь*
$ ell_"total" = ell_"pred" + lambda ( mu_1 sum_(l=0)^(L-1) norm(Phi_l)_1 + mu_2 sum_(l=0)^(L-1) S(Phi_l) ), $

где $mu_1, mu_2$ — относительные масштабы, обычно установленные как $mu_1 = mu_2 = 1$,
а $lambda$ контролирует общую величину регуляризации.

